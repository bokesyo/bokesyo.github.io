<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://bokesyo.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://bokesyo.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-12-21T05:08:03+00:00</updated><id>https://bokesyo.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Probe conciousness with multimodal long-context large language models</title><link href="https://bokesyo.github.io/blog/2023/llm-agents/" rel="alternate" type="text/html" title="Probe conciousness with multimodal long-context large language models"/><published>2023-12-15T16:40:16+00:00</published><updated>2023-12-15T16:40:16+00:00</updated><id>https://bokesyo.github.io/blog/2023/llm-agents</id><content type="html" xml:base="https://bokesyo.github.io/blog/2023/llm-agents/"><![CDATA[<p>In my Cognitive Psychology course at CUHK(SZ) — for which I received a grade of C+ — I was first introduced to the concept of <em>perception</em>. It’s a fascinating property found in myriad entities. For instance, after pretraining, a Vision Transformer can “perceive” a <em>cat</em> within its embeddings. The property also exists in many creatures (insects, cats, dogs, human).</p> <p>I frequently muse about the essence of <em>human consciousness</em>—potentially defined as the brain’s perception of its existence. It’s a property absent in many entities. Take a bottle, for instance. Can it perceive its existence? This form of consciousness varies among beings—comparable in mammals, yet distinct in insects. I’m curious: if we were to replicate such a brain structure using VISOR and extract circuits using Deep Learning, and then program these networks to process signals, what could be the outcome?</p> <p>On a parallel note, imagine using a large language model paired with vision or environmental encoders and external devices. If this model had an extensive context capacity (like 1,000,000+ tokens) to retain daily information, and if trained to embed these memories into its parameters, what could be the possibilities? And, the objective function, should be somewhat like <code class="language-plaintext highlighter-rouge">uncertainty</code>, because neurons prefer a predictable stimulus, rather than a random stimulus.</p>]]></content><author><name></name></author><category term="thoughts"/><category term="thoughts"/><summary type="html"><![CDATA[Imagine using a large language model paired with vision or environmental encoders and external devices.]]></summary></entry><entry><title type="html">An explanation for conciousness</title><link href="https://bokesyo.github.io/blog/2023/cliques-conciousness/" rel="alternate" type="text/html" title="An explanation for conciousness"/><published>2023-11-01T16:40:16+00:00</published><updated>2023-11-01T16:40:16+00:00</updated><id>https://bokesyo.github.io/blog/2023/cliques-conciousness</id><content type="html" xml:base="https://bokesyo.github.io/blog/2023/cliques-conciousness/"><![CDATA[<p>Recently I came across an explanation for <em>conciousness</em> in cerebral cortex. The cortex, could be represented by multiple neuron cliques, each clique could encode a type of unique feeling, and these cliques could be varying, that means, during learning, the cliques and the number of cliques could change. When multiple cliques activates simutanuously, it could produce different combinations of feelings, that becomes the conciousness. Just like the hidden states of a large language model, usually <code class="language-plaintext highlighter-rouge">4096</code> dimensional, could represent the overall semantic for a sentence or a passage. But now the problem is, how do we <code class="language-plaintext highlighter-rouge">perceive</code> our own existence? That could be a problem… But the <code class="language-plaintext highlighter-rouge">perception of our own existence</code> could also be a type of feeling (semantic) composed by a combination of neuron cliques. So, that could provide a possible explanation of <code class="language-plaintext highlighter-rouge">conciousness</code> defined above. And the human awareness is nothing different from other mammals or insects. It is just because insects’ brain is not complex enought to perceive their own existence. But the perception is the same.</p>]]></content><author><name></name></author><category term="thoughts"/><category term="thoughts"/><summary type="html"><![CDATA[I proposed an explanation for conciousness in cerebral cortex, building a link between language models and human brains.]]></summary></entry></feed>