---
layout: about
title: about
permalink: /
subtitle: <a href='https://www.cuhk.edu.cn/en/about-us'>The Chinese University of Hong Kong, Shenzhen</a>

profile:
  align: right
  image: my_pic.jpg
  image_circular: false # crops the image to make it circular

# news: true  # includes a list of news items
latest_posts: true  # includes a list of the newest posts
selected_papers: true # includes a list of papers marked as "selected={true}"
social: true  # includes social icons at the bottom of the page
---

I am a 5th-year undergrad @ CUHKSZ. My passion is understanding the ties between learning in neural networks and in biological brain. I love industrial research in that. I don't pursue Ph.D. 

Additionally, I feel it cool to investigate Chinese dialects. 

## recent work

**ICBI@SIAT BCBDI**

At [ICBI@SIAT BCBDI](http://bcbdi.siat.ac.cn/), I'm immersed in the realm of brain reconstruction. Based on VISOR2 fast imaging technology developed by ICBI, we can rapidly obtain images of whole mouse / macaque / human brains.

- Pretrained a ViT encoder & decoder (through Masked Autoencoder).
  
- Aligned pretrained Masked Autoencoder for trustfully stitching brain slices.

- The ultimate aim is to extracting whole brain high-resolution circuits.


**THUNLP**

At [THUNLP@THU](https://nlp.csai.tsinghua.edu.cn/), my pursuit centers on enhancing the mathematical reasoning capability of large language models.

- Prior to this, I contributed to the creation of BMQA (Big Model for Question Answering, June, 2022) & [UltraLM](https://github.com/thunlp/UltraChat).

- Also [BMTools](https://github.com/OpenBMB/BMTools) & [GPT-World](https://github.com/ShengdingHu/GPT-World), where LLMs are treated as intelligent agents.


**Zealen Technology (Startup)**

I architected a suite of Swin Transformer-based wind power forecast models. 


## Research interest

In my Cognitive Psychology course at CUHK(SZ) — for which I received a grade of C+ — I was first introduced to the concept of *perception*. It's a fascinating property found in myriad entities. For instance, after pretraining, a Vision Transformer can "perceive" a *cat* within its embeddings. The property also exists in many creatures (insects, cats, dogs, human).

I frequently muse about the essence of *human consciousness*—potentially defined as the brain's perception of its existence. It's a property absent in many entities. Take a bottle, for instance. Can it perceive its existence? This form of consciousness varies among beings—comparable in mammals, yet distinct in insects. I'm curious: if we were to replicate such a brain structure using VISOR and extract circuits using Deep Learning, and then program these networks to process signals, what could be the outcome?

On a parallel note, imagine using a large language model paired with vision or environmental encoders and external devices. If this model had an extensive context capacity (like 1,000,000+ tokens) to retain daily information, and if trained to embed these memories into its parameters, what could be the possibilities? And, the objective function, should be somewhat like `uncertainty`, because neurons prefer a predictable stimulus, rather than a random stimulus.

My vision:

1. Understanding large language models' training dynamics, and knowledge storage mechanism.

2. Understanding whether there is a matching between biological brain and large language models, by extracting circuits from brain imaging.

